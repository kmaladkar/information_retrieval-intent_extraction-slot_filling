{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "german-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "settled-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "woz_directory = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "pressing-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processor(split):\n",
    "    with open(woz_directory + f\"WOZ_{split}_utt.txt\") as f1:\n",
    "        data_utts = [s.strip() for s in f1.readlines()]\n",
    "    with open(woz_directory + f\"WOZ_{split}_ans.txt\") as f2:\n",
    "        data_ans = [s.strip() for s in f2.readlines()]\n",
    "    data_ans_first = []\n",
    "    data_ans_second = []\n",
    "    for ans in data_ans:\n",
    "        ans_split = ans.split(\"|\")\n",
    "        type_dict = {}\n",
    "        data_ans_first.append(ans_split[0])\n",
    "        for type_ in ans_split[1:]:\n",
    "            type_split = type_.split(\"-\")[1].split(\"=\")\n",
    "            type_dict[type_split[0]] = type_split[1]\n",
    "        data_ans_second.append(type_dict)\n",
    "    \n",
    "    data_find_hotel_ids = [idx for idx, ans in enumerate(data_ans) if ans.split(\"|\")[0] == \"find_hotel\"]\n",
    "    data_hotel_ans = np.array(data_ans_second)[data_find_hotel_ids]\n",
    "    data_hotel_utts = np.array(data_utts)[data_find_hotel_ids]\n",
    "    hotel_df = pd.DataFrame.from_dict(list(data_hotel_ans)).fillna(\"\")\n",
    "    hotel_df['utts'] = data_hotel_utts\n",
    "    \n",
    "    data_find_rest_ids = [idx for idx, ans in enumerate(data_ans) if ans.split(\"|\")[0] == \"find_restaurant\"]\n",
    "    data_rest_ans = np.array(data_ans_second)[data_find_rest_ids]\n",
    "    data_rest_utts = np.array(data_utts)[data_find_rest_ids]\n",
    "    rest_df = pd.DataFrame.from_dict(list(data_rest_ans)).fillna(\"\")\n",
    "    rest_df['utts'] = data_rest_utts\n",
    "    \n",
    "    hotel_df.to_csv(f\"./data/{split}_hotel.csv\", index=False)\n",
    "    rest_df.to_csv(f\"./data/{split}_restaurant.csv\", index=False)\n",
    "    \n",
    "    h = [\"find_hotel\" for _ in range(hotel_df.shape[0])]\n",
    "    r = [\"find_restaurant\" for _ in range(rest_df.shape[0])]\n",
    "    df = pd.DataFrame({\"utts\": list(data_hotel_utts) + list(data_rest_utts), \"labels\": h + r})\n",
    "    df.to_csv(f\"./data/{split}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "hungarian-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "wicked-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-wagner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ceramic-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bies(sentence, tags, type_, curr_tags):\n",
    "    tags = tags.split()\n",
    "    check = False\n",
    "    idx = 0\n",
    "    for ix, word in enumerate(sentence):\n",
    "        if (idx < len(tags)) and (word == tags[idx]):\n",
    "            if idx == 0:\n",
    "                curr_tags[ix] = \"B-\"+type_\n",
    "            else:\n",
    "                curr_tags[ix] = \"I-\"+type_\n",
    "            idx += 1\n",
    "        \n",
    "    return curr_tags\n",
    "\n",
    "def get_labels_dict(label):\n",
    "    label_dict = {}\n",
    "    label_splits = label.split(\"|\")[1:]\n",
    "    for s in label_splits:\n",
    "        key = s.split(\"-\")[1].split(\"=\")[0]\n",
    "        tag = s.split(\"-\")[1].split(\"=\")[1]\n",
    "        label_dict[key] = tag\n",
    "    return label_dict\n",
    "\n",
    "def sentence2iob(sentence, label_dict):\n",
    "    sentence = sentence.lower().split()\n",
    "    curr_tags = [\"O\" for _ in range(len(sentence))]\n",
    "    for key, value in label_dict.items():\n",
    "        if type(value) == str:\n",
    "            curr_tags = get_bies(sentence, value, key, curr_tags)\n",
    "    sentence.append(\"[SEP]\")\n",
    "    curr_tags.append(\"[SEP]\")\n",
    "    sentence.extend([\"area\", \"pricerange\", \"type\", \"internet\", \"parking\", \"stars\"])\n",
    "    curr_tags.extend([\"O\", \"O\", \"O\", \"O\", \"O\", \"O\"])\n",
    "    value_dict = {\"area\": -6, \"pricerange\": -5, \"type\": -4, \"internet\": -3, \"parking\": -2, \"stars\": -1}\n",
    "    for key, value in label_dict.items():\n",
    "        if key in [\"area\", \"pricerange\", \"type\", \"internet\", \"parking\", \"stars\"]:\n",
    "            curr_tags[value_dict[key]] = value\n",
    "    return sentence, curr_tags\n",
    "\n",
    "all_tokens = []\n",
    "all_tags = []\n",
    "all_dicts = []\n",
    "with open(\"./data/WOZ_train_utt.txt\", \"r\") as f1, open(\"./data/WOZ_train_ans.txt\", \"r\") as f2:\n",
    "    sentences = f1.readlines()\n",
    "    labels = f2.readlines()\n",
    "    for sentence, labels in zip(sentences, labels):\n",
    "        sentence = sentence.strip()\n",
    "        labels = labels.strip()\n",
    "        label_dict = get_labels_dict(labels)\n",
    "        tokens, tags = sentence2iob(sentence, label_dict)\n",
    "        all_tokens.append(tokens)\n",
    "        all_tags.append(tags)\n",
    "        all_dicts.append(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "quantitative-knife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Guten', 'Tag,', 'I', 'am', 'staying', 'overnight', 'in', 'Cambridge', 'and', 'need', 'a', 'place', 'to', 'sleep.', 'I', 'need', 'free', 'parking', 'and', 'internet.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "{'area': 'centre', 'internet': 'yes', 'parking': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(all_tokens[i])\n",
    "print(all_tags[i])\n",
    "print(all_dicts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "stuck-peripheral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-essay",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ubcmds]",
   "language": "python",
   "name": "conda-env-ubcmds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
