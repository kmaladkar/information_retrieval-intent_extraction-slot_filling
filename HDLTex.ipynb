{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "detected-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchtext\n",
    "from torchtext.data import Field, Dataset, BucketIterator\n",
    "import pandas as pd\n",
    "from DataFrameDataSet import DataFrameDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.functional import log_softmax, relu\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "from random import random, seed, shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "burning-canadian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guten Tag, I am staying overnight in Cambridge...</td>\n",
       "      <td>find_hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi there! Can you give me some info on Cityroomz?</td>\n",
       "      <td>find_hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am looking for a hotel named alyesbray lodge...</td>\n",
       "      <td>find_hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm looking for a places to go and see during ...</td>\n",
       "      <td>find_hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I need a place to stay that has free wifi.</td>\n",
       "      <td>find_hotel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                utts      labels\n",
       "0  Guten Tag, I am staying overnight in Cambridge...  find_hotel\n",
       "1  Hi there! Can you give me some info on Cityroomz?  find_hotel\n",
       "2  I am looking for a hotel named alyesbray lodge...  find_hotel\n",
       "3  I'm looking for a places to go and see during ...  find_hotel\n",
       "4         I need a place to stay that has free wifi.  find_hotel"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "stupid-divorce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ubcmds/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "PAD=\"<pad>\"\n",
    "UNK=\"<unk>\"\n",
    "START=\"<start>\"\n",
    "END=\"<end>\"\n",
    "\n",
    "TEXT = Field(\n",
    "    sequential=True,\n",
    "    init_token = START,\n",
    "    eos_token=END,\n",
    "    pad_token=PAD,\n",
    "    unk_token=UNK,\n",
    "    tokenize=lambda x : x.lower().split())\n",
    "LABEL = Field(sequential=False, unk_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "warming-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {'labels' : LABEL,'utts' : TEXT}\n",
    "train = DataFrameDataset(train_df, fields)\n",
    "\n",
    "TEXT.build_vocab(train) \n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "governing-software",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ubcmds/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "train_iter = BucketIterator(\n",
    "        train,\n",
    "        batch_size=16,\n",
    "        device=\"cpu\",\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "sufficient-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reproducible results.\n",
    "seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "  \n",
    "  def __init__(self, embedding_size, vocab_size, output_size, hidden_size, num_layers):\n",
    "    super(BiLSTM, self).__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
    "    self.rnn_layer = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True) \n",
    "    self.activation_fn = nn.ReLU()\n",
    "    self.linear_layer = nn.Linear(hidden_size*2, output_size) \n",
    "    self.softmax_layer = nn.LogSoftmax(dim=1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out = self.embedding(x)\n",
    "    out, _ = self.rnn_layer(out) # since we are not feeding h_0 explicitly, h_0 will be initialized to zeros by default\n",
    "    out = out[-1]\n",
    "    out = self.activation_fn(out)\n",
    "    out = self.linear_layer(out)\n",
    "    out = self.softmax_layer(out) # accepts 2D or more dimensional inputs\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "broke-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(embedding_size=100, vocab_size=len(TEXT.vocab.stoi), output_size=2, hidden_size=100, num_layers=1)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "adopted-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_function(loader):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_sample = 0\n",
    "    for batch in loader:\n",
    "        # load the current batch\n",
    "        batch_input = batch.utts\n",
    "        batch_output = batch.labels\n",
    "        \n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output = batch_output.to(device)\n",
    "        # forward propagation\n",
    "        # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_sample += batch_output.shape[0]\n",
    "    return total_loss/num_sample\n",
    "\n",
    "def evaluate(loader):\n",
    "    all_pred=[]\n",
    "    all_label = []\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "             # load the current batch\n",
    "            batch_input = batch.utts\n",
    "            batch_output = batch.labels\n",
    "\n",
    "            batch_input = batch_input.to(device)\n",
    "            # forward propagation\n",
    "            # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "            # identify the predicted class for each example in the batch\n",
    "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
    "            # put all the true labels and predictions to two lists\n",
    "            all_pred.extend(predicted)\n",
    "            all_label.extend(batch_output)\n",
    "            \n",
    "    accuracy = accuracy_score(all_label, all_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "irish-introduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0615, Training Accuracy: 0.4279\n",
      "Epoch [2/10], Loss: 0.0499, Training Accuracy: 0.4354\n",
      "Epoch [3/10], Loss: 0.0447, Training Accuracy: 0.7045\n",
      "Epoch [4/10], Loss: 0.0398, Training Accuracy: 0.7037\n",
      "Epoch [5/10], Loss: 0.0401, Training Accuracy: 0.7431\n",
      "Epoch [6/10], Loss: 0.0370, Training Accuracy: 0.7824\n",
      "Epoch [7/10], Loss: 0.0373, Training Accuracy: 0.5822\n",
      "Epoch [8/10], Loss: 0.0399, Training Accuracy: 0.7968\n",
      "Epoch [9/10], Loss: 0.0372, Training Accuracy: 0.7731\n",
      "Epoch [10/10], Loss: 0.0376, Training Accuracy: 0.7178\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(MAX_EPOCHS):\n",
    "    # train the model for one pass over the data\n",
    "    train_loss = train_function(train_iter)  \n",
    "    # compute the training accuracy\n",
    "    train_acc = evaluate(train_iter)\n",
    "    # compute the validation accuracy\n",
    "#     val_acc = evaluate(val_iter)\n",
    "    \n",
    "    # print the loss for every epoch\n",
    "#     print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss, train_acc, val_acc))\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss, train_acc))\n",
    "    \n",
    "    # save model, optimizer, and number of epoch to a dictionary\n",
    "    model_save = {\n",
    "            'epoch': epoch,  # number of epoch\n",
    "            'model_state_dict': model.state_dict(), # model parameters \n",
    "            'optimizer_state_dict': optimizer.state_dict(), # save optimizer \n",
    "            'loss': train_loss # training loss\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-final",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ubcmds]",
   "language": "python",
   "name": "conda-env-ubcmds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
